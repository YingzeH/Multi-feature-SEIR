{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7U9Tujta2TXF6A1rC5maa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YingzeH/Multi-feature-SEIR/blob/main/RL_Grid_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "gW5jV1AX9GuA"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simlple grid game, goal is to reach the bottom corner"
      ],
      "metadata": {
        "id": "cNnt2xFqMLca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridEnvironment:\n",
        "    def __init__(self, n, m, start=(0, 0)):\n",
        "        self.n = n  # Number of rows\n",
        "        self.m = m  # Number of columns\n",
        "        self.goal = (n - 1, m - 1)  # Goal position at the bottom-right corner\n",
        "        self.state = start  # Initialize the state to a specified starting position\n",
        "\n",
        "    def reset(self, start=(0, 0)):\n",
        "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
        "        self.state = start  # Reset to the specified starting position\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Executes the given action and returns the new state, reward, and done flag.\"\"\"\n",
        "        x, y = self.state\n",
        "\n",
        "        # Define action behavior: 0=up, 1=down, 2=left, 3=right\n",
        "        if action == 0 and y > 0:  # Move up\n",
        "            y -= 1\n",
        "        elif action == 1 and y < self.n - 1:  # Move down\n",
        "            y += 1\n",
        "        elif action == 2 and x > 0:  # Move left\n",
        "            x -= 1\n",
        "        elif action == 3 and x < self.m - 1:  # Move right\n",
        "            x += 1\n",
        "\n",
        "        # Update state\n",
        "        self.state = (x, y)\n",
        "\n",
        "        # Check if the goal is reached\n",
        "        if self.is_goal_reached():\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.01  # Penalty for each step to encourage shorter paths\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def is_goal_reached(self):\n",
        "        \"\"\"Checks if the current state is the goal.\"\"\"\n",
        "        return self.state == self.goal\n",
        "\n",
        "    def get_action_space(self):\n",
        "        \"\"\"Returns the number of possible actions (4: up, down, left, right).\"\"\"\n",
        "        return 4\n",
        "\n",
        "    def get_state_space(self):\n",
        "        \"\"\"Returns the state space dimensions (n x m).\"\"\"\n",
        "        return self.n, self.m\n"
      ],
      "metadata": {
        "id": "5YyGn-Hg9IGJ"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, environment, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.env = environment\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.epsilon_decay = epsilon_decay  # Epsilon decay rate\n",
        "        self.epsilon_min = epsilon_min  # Minimum exploration rate\n",
        "        self.q_table = np.zeros((self.env.n, self.env.m, self.env.get_action_space()))  # Initialize Q-table\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Choose an action using an epsilon-greedy strategy.\"\"\"\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, self.env.get_action_space() - 1)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Exploit\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        \"\"\"Update the Q-table based on the action taken and reward received.\"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n",
        "        td_delta = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += self.alpha * td_delta\n",
        "\n",
        "    def train(self, episodes):\n",
        "      \"\"\"Train the agent over a number of episodes.\"\"\"\n",
        "      total_reward = 0  # To accumulate total reward for each episode\n",
        "      rewards = []  # List to keep track of rewards per episode\n",
        "\n",
        "      for episode in range(episodes):\n",
        "          state = self.env.reset()  # Reset the environment for a new episode\n",
        "          done = False\n",
        "          episode_reward = 0  # Reward for the current episode\n",
        "\n",
        "          while not done:\n",
        "            action = self.choose_action(state)  # Choose action\n",
        "            next_state, reward, done = self.env.step(action)  # Take action in the environment\n",
        "            self.update_q_table(state, action, reward, next_state)  # Update Q-table\n",
        "            state = next_state  # Transition to the next state\n",
        "\n",
        "            episode_reward += reward  # Accumulate reward for this episode\n",
        "\n",
        "          rewards.append(episode_reward)  # Store the total reward for this episode\n",
        "          total_reward += episode_reward\n",
        "\n",
        "          # Decay epsilon after each episode\n",
        "          if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "          # Print progress every 100 episodes\n",
        "          if (episode + 1) % 100 == 0:\n",
        "            avg_reward = total_reward / 100  # Average reward over the last 100 episodes\n",
        "            print(f\"Episode {episode + 1}/{episodes}, Average Reward: {avg_reward:.2f}\")\n",
        "            total_reward = 0  # Reset total reward for the next average calculation\n",
        "\n",
        "\n",
        "    def print_q_table(self):\n",
        "        \"\"\"Prints the Q-table in a formatted way.\"\"\"\n",
        "        print(\"State       Up           Down         Left         Right\")\n",
        "        print(\"---------------------------------------------------------------\")\n",
        "        for i in range(self.env.n):\n",
        "            for j in range(self.env.m):\n",
        "                state = (i, j)\n",
        "                q_values = self.q_table[i, j]\n",
        "                # Convert state tuple to a formatted string\n",
        "                state_str = str(state).ljust(12)\n",
        "                print(f\"{state_str} {q_values[0]:<12.2f} {q_values[1]:<12.2f} {q_values[2]:<12.2f} {q_values[3]:<12.2f}\")\n",
        "\n",
        "    def get_optimal_path(self, start):\n",
        "        \"\"\"Returns the optimal path from the given starting point to the goal.\"\"\"\n",
        "        path = [start]\n",
        "        state = start\n",
        "\n",
        "        while state != self.env.goal:\n",
        "            action = np.argmax(self.q_table[state])  # Choose the best action based on Q-values\n",
        "            if action == 0:  # Up\n",
        "                state = (state[0], state[1] - 1) if state[1] > 0 else state\n",
        "            elif action == 1:  # Down\n",
        "                state = (state[0], state[1] + 1) if state[1] < self.env.n - 1 else state\n",
        "            elif action == 2:  # Left\n",
        "                state = (state[0] - 1, state[1]) if state[0] > 0 else state\n",
        "            elif action == 3:  # Right\n",
        "                state = (state[0] + 1, state[1]) if state[0] < self.env.m - 1 else state\n",
        "\n",
        "            path.append(state)  # Add the new state to the path\n",
        "\n",
        "        return path\n",
        "\n"
      ],
      "metadata": {
        "id": "q_yxXhp1-MMr"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "n, m = 3, 3  # Grid size\n",
        "env = GridEnvironment(n, m)\n",
        "agent = QLearningAgent(env)\n",
        "\n",
        "# Train the agent\n",
        "agent.train(1000)  # Train for 1000 episodes\n",
        "\n",
        "# Print the Q-table\n",
        "agent.print_q_table()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaArwrCVISO3",
        "outputId": "41637af0-b5ef-4143-dae1-303f1e985b67"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100/1000, Average Reward: 0.88\n",
            "Episode 200/1000, Average Reward: 0.95\n",
            "Episode 300/1000, Average Reward: 0.95\n",
            "Episode 400/1000, Average Reward: 0.96\n",
            "Episode 500/1000, Average Reward: 0.97\n",
            "Episode 600/1000, Average Reward: 0.97\n",
            "Episode 700/1000, Average Reward: 0.97\n",
            "Episode 800/1000, Average Reward: 0.97\n",
            "Episode 900/1000, Average Reward: 0.97\n",
            "Episode 1000/1000, Average Reward: 0.97\n",
            "State       Up           Down         Left         Right\n",
            "---------------------------------------------------------------\n",
            "(0, 0)       0.62         0.70         0.62         0.70        \n",
            "(0, 1)       0.52         0.66         0.64         0.79        \n",
            "(0, 2)       0.50         0.55         0.58         0.89        \n",
            "(1, 0)       0.70         0.79         0.61         0.77        \n",
            "(1, 1)       0.70         0.89         0.70         0.88        \n",
            "(1, 2)       0.79         0.89         0.79         1.00        \n",
            "(2, 0)       0.45         0.89         0.50         0.52        \n",
            "(2, 1)       0.56         1.00         0.61         0.49        \n",
            "(2, 2)       0.00         0.00         0.00         0.00        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input starting coordinates\n",
        "start_x = int(input(\"Enter starting x coordinate (0 to {}): \".format(n-1)))\n",
        "start_y = int(input(\"Enter starting y coordinate (0 to {}): \".format(m-1)))\n",
        "\n",
        "# Output the optimal path from the given starting point\n",
        "optimal_path = agent.get_optimal_path((start_x, start_y)) # Pass start_x and start_y as a tuple\n",
        "print(\"Optimal Path:\", optimal_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJoTFEldJPL1",
        "outputId": "b2b04963-ab86-4392-be0c-76b226edb6a7"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter starting x coordinate (0 to 2): 0\n",
            "Enter starting y coordinate (0 to 2): 1\n",
            "Optimal Path: [(0, 1), (1, 1), (1, 2), (2, 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are traps in the diagnoles, try to avoid them!"
      ],
      "metadata": {
        "id": "G9N1qXW8MW23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridEnvironment:\n",
        "    def __init__(self, n, m, start=(0, 0)):\n",
        "        self.n = n  # Number of rows\n",
        "        self.m = m  # Number of columns\n",
        "        self.goal = (n - 1, m - 1)  # Goal position at the bottom-right corner\n",
        "        self.state = start  # Initialize the state to a specified starting position\n",
        "        self.traps = {(i, i) for i in range(1, min(n, m) - 1)}    # Set of trap positions on the diagonal\n",
        "\n",
        "        # Check if the starting position is a trap\n",
        "        if self.state in self.traps:\n",
        "            raise ValueError(\"Starting position is a trap! Game terminates.\")\n",
        "\n",
        "    def reset(self, start=(0, 0)):\n",
        "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
        "        self.state = start  # Reset to the specified starting position\n",
        "\n",
        "        # Check if the new starting position is a trap\n",
        "        if self.state in self.traps:\n",
        "            raise ValueError(\"Starting position is a trap! Game terminates.\")\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Executes the given action and returns the new state, reward, and done flag.\"\"\"\n",
        "        x, y = self.state\n",
        "\n",
        "        # Define action behavior: 0=up, 1=down, 2=left, 3=right\n",
        "        if action == 0 and y > 0:  # Move up\n",
        "            y -= 1\n",
        "        elif action == 1 and y < self.n - 1:  # Move down\n",
        "            y += 1\n",
        "        elif action == 2 and x > 0:  # Move left\n",
        "            x -= 1\n",
        "        elif action == 3 and x < self.m - 1:  # Move right\n",
        "            x += 1\n",
        "\n",
        "        # Update state\n",
        "        self.state = (x, y)\n",
        "\n",
        "        # Check for traps\n",
        "        if self.state in self.traps:\n",
        "            reward = -1  # Penalty for landing on a trap\n",
        "            done = True  # The game ends when hitting a trap\n",
        "            print(\"Landed in a trap! Game terminates.\")\n",
        "        elif self.is_goal_reached():\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.01  # Penalty for each step to encourage shorter paths\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def is_goal_reached(self):\n",
        "        \"\"\"Checks if the current state is the goal.\"\"\"\n",
        "        return self.state == self.goal\n",
        "\n",
        "    def get_action_space(self):\n",
        "        \"\"\"Returns the number of possible actions (4: up, down, left, right).\"\"\"\n",
        "        return 4\n",
        "\n",
        "    def get_state_space(self):\n",
        "        \"\"\"Returns the state space dimensions (n x m).\"\"\"\n",
        "        return self.n, self.m\n"
      ],
      "metadata": {
        "id": "82k8ZB7cJwHT"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, environment, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.env = environment\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.epsilon_decay = epsilon_decay  # Epsilon decay rate\n",
        "        self.epsilon_min = epsilon_min  # Minimum exploration rate\n",
        "        self.q_table = np.zeros((self.env.n, self.env.m, self.env.get_action_space()))  # Initialize Q-table\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Choose an action using an epsilon-greedy strategy.\"\"\"\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, self.env.get_action_space() - 1)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Exploit\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        \"\"\"Update the Q-table based on the action taken and reward received.\"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n",
        "        td_delta = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += self.alpha * td_delta\n",
        "\n",
        "    def train(self, episodes):\n",
        "        \"\"\"Train the agent over a number of episodes.\"\"\"\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()  # Reset the environment for a new episode\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.choose_action(state)  # Choose action\n",
        "                next_state, reward, done = self.env.step(action)  # Take action in the environment\n",
        "                self.update_q_table(state, action, reward, next_state)  # Update Q-table\n",
        "                state = next_state  # Transition to the next state\n",
        "\n",
        "            # Decay epsilon after each episode\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= self.epsilon_decay\n",
        "\n",
        "            # Output training progress\n",
        "            if (episode + 1) % 100 == 0:  # Every 100 episodes\n",
        "                print(f\"Episode {episode + 1}/{episodes}, Epsilon: {self.epsilon:.4f}\")\n",
        "\n",
        "    def print_q_table(self):\n",
        "        \"\"\"Prints the Q-table in a formatted way.\"\"\"\n",
        "        print(\"State       Up           Down         Left         Right\")\n",
        "        print(\"---------------------------------------------------------------\")\n",
        "        for i in range(self.env.n):\n",
        "            for j in range(self.env.m):\n",
        "                state = (i, j)\n",
        "                q_values = self.q_table[i, j]\n",
        "                # Convert state tuple to a formatted string\n",
        "                state_str = str(state).ljust(12)\n",
        "                print(f\"{state_str} {q_values[0]:<12.2f} {q_values[1]:<12.2f} {q_values[2]:<12.2f} {q_values[3]:<12.2f}\")\n",
        "\n",
        "    def get_optimal_path(self, start):\n",
        "        \"\"\"Returns the optimal path from the given starting point to the goal.\"\"\"\n",
        "        path = [start]\n",
        "        state = start\n",
        "\n",
        "        while state != self.env.goal:\n",
        "            action = np.argmax(self.q_table[state])  # Choose the best action based on Q-values\n",
        "            if action == 0:  # Up\n",
        "                state = (state[0], state[1] - 1) if state[1] > 0 else state\n",
        "            elif action == 1:  # Down\n",
        "                state = (state[0], state[1] + 1) if state[1] < self.env.n - 1 else state\n",
        "            elif action == 2:  # Left\n",
        "                state = (state[0] - 1, state[1]) if state[0] > 0 else state\n",
        "            elif action == 3:  # Right\n",
        "                state = (state[0] + 1, state[1]) if state[0] < self.env.m - 1 else state\n",
        "\n",
        "            path.append(state)  # Add the new state to the path\n",
        "\n",
        "        return path\n"
      ],
      "metadata": {
        "id": "NVxz91meMdvV"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "n, m = 5, 5  # Grid size\n",
        "env = GridEnvironment(n, m)\n",
        "agent = QLearningAgent(env)\n",
        "\n",
        "# Train the agent\n",
        "agent.train(1000)  # Train for 1000 episodes\n",
        "\n",
        "# Print the Q-table\n",
        "agent.print_q_table()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoZ3PC0ENJi9",
        "outputId": "6c790822-0791-4f13-af58-2e719034d18f"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 100/1000, Epsilon: 0.6058\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 200/1000, Epsilon: 0.3670\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 300/1000, Epsilon: 0.2223\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 400/1000, Epsilon: 0.1347\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 500/1000, Epsilon: 0.0816\n",
            "Landed in a trap! Game terminates.\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 600/1000, Epsilon: 0.0494\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 700/1000, Epsilon: 0.0299\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 800/1000, Epsilon: 0.0181\n",
            "Episode 900/1000, Epsilon: 0.0110\n",
            "Landed in a trap! Game terminates.\n",
            "Episode 1000/1000, Epsilon: 0.0100\n",
            "State       Up           Down         Left         Right\n",
            "---------------------------------------------------------------\n",
            "(0, 0)       0.31         0.14         0.28         0.43        \n",
            "(0, 1)       0.29         -0.02        -0.03        -0.98       \n",
            "(0, 2)       -0.02        -0.01        -0.02        -0.02       \n",
            "(0, 3)       -0.02        0.03         -0.01        -0.00       \n",
            "(0, 4)       -0.01        -0.01        -0.00        0.12        \n",
            "(1, 0)       0.35         -1.00        0.31         0.48        \n",
            "(1, 1)       0.00         0.00         0.00         0.00        \n",
            "(1, 2)       -0.47        -0.00        -0.01        -0.19       \n",
            "(1, 3)       -0.01        0.09         -0.01        -0.01       \n",
            "(1, 4)       0.00         0.02         -0.00        0.31        \n",
            "(2, 0)       0.44         0.34         0.25         0.55        \n",
            "(2, 1)       0.03         -0.69        -0.47        0.55        \n",
            "(2, 2)       0.00         0.00         0.00         0.00        \n",
            "(2, 3)       -0.27        -0.00        -0.00        -0.34       \n",
            "(2, 4)       -0.00        0.04         0.01         0.58        \n",
            "(3, 0)       0.43         0.54         0.41         0.62        \n",
            "(3, 1)       0.28         0.70         0.12         0.35        \n",
            "(3, 2)       0.18         -0.65        -0.72        0.79        \n",
            "(3, 3)       0.00         0.00         0.00         0.00        \n",
            "(3, 4)       -0.34        0.20         0.01         0.89        \n",
            "(4, 0)       0.54         0.70         0.41         0.57        \n",
            "(4, 1)       0.55         0.79         0.45         0.62        \n",
            "(4, 2)       0.59         0.89         0.68         0.76        \n",
            "(4, 3)       0.73         1.00         -0.94        0.63        \n",
            "(4, 4)       0.00         0.00         0.00         0.00        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input starting coordinates\n",
        "start_x = int(input(\"Enter starting x coordinate (0 to {}): \".format(n-1)))\n",
        "start_y = int(input(\"Enter starting y coordinate (0 to {}): \".format(m-1)))\n",
        "\n",
        "# Check if the entered starting position is a trap\n",
        "if (start_x, start_y) in env.traps:\n",
        "    print(\"Starting position is a trap! Game terminates.\")\n",
        "else:\n",
        "    # Output the optimal path from the given starting point\n",
        "    optimal_path = agent.get_optimal_path((start_x, start_y))  # Pass start_x and start_y as a tuple\n",
        "    print(\"Optimal Path:\", optimal_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeFEE-RSNPS8",
        "outputId": "69f800b2-0599-40a2-92c5-5ce7451c29d3"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter starting x coordinate (0 to 4): 0\n",
            "Enter starting y coordinate (0 to 4): 2\n",
            "Optimal Path: [(0, 2), (0, 3), (0, 4), (1, 4), (2, 4), (3, 4), (4, 4)]\n"
          ]
        }
      ]
    }
  ]
}